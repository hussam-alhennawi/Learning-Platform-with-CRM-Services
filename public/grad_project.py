# -*- coding: utf-8 -*-
"""Grad Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C9LKdgBKfkknbfJf-vIUSfPMm8zXaYLm
"""

import re
from nltk.corpus import stopwords
import nltk
from nltk.util import ngrams
import numpy as np
import pandas as pd
from nltk import FreqDist

"""## Get Data"""

#with open('complaints.xlsx', 'r', encoding='utf-8-sig') as f:
# all_complaints = f.read()
all_complaints = pd.read_excel('complaints.xlsx' , encoding = 'utf-8') 
#all_complaints
data_needed = all_complaints["complaints"]
data_needed

def clean_sentence(sent):
    sent = sent.replace('أ', 'ا')
    sent = sent.replace('ة', 'ه')
    sent = sent.replace('ؤ', 'ء')
    sent = sent.replace('ئ', 'ء')
    return sent

nltk.download('stopwords')

def remove_stop_words(sent):
    sent_list = sent.split(' ')
    sent_list = [word for word in sent_list if word not in stopwords.words('arabic')]
    return sent_list

"""### Clean Data"""

clean_data = []
for com in data_needed:
    tmp = clean_sentence(com)
    tmp = remove_stop_words(tmp)
    tmp = [word for word in tmp if word != '']
    clean_data.append(tmp)

data = []
for sent in clean_data:
    data.append(' '.join(sent))
#data
len(data)
type(data)

import json 
  
# Data to be written 
dictionary ={ 
    "exam" : 60
} 
dictionary["complaint"] = 40
  
# Serializing json  
json_object = json.dumps(dictionary, indent = 4) 
  
# Writing to sample.json 
with open("sample.json", "w") as outfile: 
    outfile.write(json_object)

"""### Universal Sentence Encoder"""

# !pip install tensorflow_text>=2.0.0rc0

import tensorflow_hub as hub
import tensorflow_text

embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')

embeddings = embed(data).numpy()

"""## Define Training Method"""

def return_one(*params):
    return 1

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

def train(model, params, X):
    pipeline = Pipeline([('CountVectorizer', CountVectorizer()), ('Model', model)])
    grid = GridSearchCV(pipeline, param_grid=params, scoring=return_one, cv=5)

    grid.fit(X)
    return grid.best_estimator_['Model'].labels_

"""## Training"""

def print_data_in_cluster(labels, cluster_num):
    for i in range(len(data)):
        if labels[i] == cluster_num:
            print(data[i])

def Labeled_data_in_cluster(labels, cluster_num):
    LabelsData = []
    for i in range(len(data)):
        if labels[i] == cluster_num:
            LabelsData.append((data[i]))
    return(LabelsData)

def return_data_in_cluster(labels, cluster_num):
    for i in range(len(data)):
        if labels[i] == cluster_num:
            return(data[i])

from nltk.corpus import stopwords
stop_words = stopwords.words('arabic')

# Commented out IPython magic to ensure Python compatibility.
# libraries for visualization

# import pyLDAvis.gensim
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# function to plot most frequent terms
def freq_words(x, terms = 30):
  all_words = ' '.join([text for text in x])
  all_words = all_words.split()

  fdist = FreqDist(all_words)
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})

  # selecting top 20 most frequent words
  d = words_df.nlargest(columns="count", n = terms) 
  plt.figure(figsize=(20,5))
  ax = sns.barplot(data=d, x= "word", y = "count")
  ax.set(ylabel = 'Count')
  plt.show()
freq_words(all_complaints['complaints'])

# function to remove stopwords
def remove_stopwords(rev):
    rev_new = " ".join([i for i in rev if i not in stop_words])
    return rev_new

# remove short words (length < 3)
all_complaints['complaints'] = all_complaints['complaints'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

# remove stopwords from the text
reviews = [remove_stopwords(r.split()) for r in all_complaints['complaints']]

# make entire text lowercase
reviews = [r.lower() for r in reviews]

freq_words(reviews, 35)

!python -m spacy download en # one time run

tokenized_reviews = pd.Series(reviews).apply(lambda x: x.split())
print(tokenized_reviews[1])
print(tokenized_reviews[2])

# Commented out IPython magic to ensure Python compatibility.
import re
import spacy

import gensim
from gensim import corpora

# libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
dictionary = corpora.Dictionary(tokenized_reviews)

doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]
# Creating the object for LDA model using gensim library
LDA = gensim.models.ldamodel.LdaModel

# Build LDA model
lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=7, random_state=100,
                chunksize=1000, passes=50)

# lda_model.print_topics()
l = lda_model.print_topics()

for item in l:
    print(item[1])

!python -m pip install -U pyLDAvis

# Visualize the topics
import pyLDAvis
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)
vis

"""#### Clustering by projection on topic vectors"""

reverse_dictionary = {}
for idx in dictionary.keys():
    reverse_dictionary[dictionary.get(idx)] = idx

clusters = {idx: [] for idx in range(lda_model.num_topics)}

def get_strength(text, cluster_vector):
    ret = 0
    for word in text:
        for term in cluster_vector:
            if reverse_dictionary[word] == term[0]:
                ret += term[1]
    
    return ret

for review in tokenized_reviews:
    max_cluster = -1
    max_cluster_index = -1
    for cluster in range(lda_model.num_topics):
        topic_vector = lda_model.get_topic_terms(cluster)
        strength = get_strength(review, topic_vector)
        if strength > max_cluster:
            max_cluster = strength
            max_cluster_index = cluster
    
    clusters[max_cluster_index].append(review)

clusters

"""### KMeans"""

from sklearn.cluster import KMeans

params = {'CountVectorizer__ngram_range': [(1, 2)],
          'Model__n_clusters': [2]}
labels = train(KMeans(), params, data)



p = return_data_in_cluster(labels, 0)
p
print_data_in_cluster(labels, 0)

params = {'CountVectorizer__ngram_range': [(1, 2)],
          'Model__n_clusters': [2]}


sent_list = p.split('\n')
sent_list

c = Labeled_data_in_cluster(labels, 0)

params = {'CountVectorizer__ngram_range': [(1, 2)],
          'Model__n_clusters': [2]}
labels = train(KMeans(), params, c)

len(c)

print_data_in_cluster(labels, 0)

"""### Affinity Propagation"""

from sklearn.cluster import AffinityPropagation

params = {'CountVectorizer__ngram_range': [(1, 2)],
          'Model__max_iter': [100]}
labels = train(AffinityPropagation(), params, data)

print_data_in_cluster(labels, 0)

"""### DBSCAN"""

from sklearn.cluster import DBSCAN

params = {'CountVectorizer__ngram_range': [(1, 2)],
          'Model__eps': [2],
          'Model__min_samples': [6]}
labels = train(DBSCAN(), params, data)

print_data_in_cluster(labels, 0)

"""### MeanShift"""

from sklearn.cluster import MeanShift, estimate_bandwidth

vectorizer = CountVectorizer(ngram_range=(1, 2))
data_vectorized = vectorizer.fit_transform(data).toarray()

bandwidth = estimate_bandwidth(data_vectorized, quantile=0.9999, n_samples=500)

meanShift = MeanShift(bandwidth=bandwidth, bin_seeding=True)
meanShift.fit(data_vectorized)
labels = meanShift.labels_
n_clusters = len(np.unique(labels))

print(n_clusters)

"""## Training on Embeddings

### KMeans
"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=8).fit(embeddings.tolist())
labels = kmeans.labels_

print_data_in_cluster(labels, 2)

"""### Plot"""

unique, counts = np.unique(labels, return_counts=True)
dictionary = dict(zip(unique, [counts]))
df = pd.DataFrame.from_dict(dictionary).reset_index().rename(columns={'index': 'cluster', 0: 'count'})

df = df.replace(to_replace=0, value='امتحان')
df = df.replace(to_replace=1, value='عملي')
df = df.replace(to_replace=2, value='مخابر')
df = df.replace(to_replace=3, value='مي')
df = df.replace(to_replace=4, value='كلب')
df = df.replace(to_replace=5, value='ميس')
df = df.replace(to_replace=6, value='ياسمين')
df = df.replace(to_replace=7, value='وجدي')

df

import plotly.express as px

px.bar(df, x='cluster', y='count', color='count')

px.pie(df, values='count', names='cluster', title='Pie Chart')



"""### Affinity Propagation"""

from sklearn.cluster import AffinityPropagation

ap = AffinityPropagation(max_iter=1000).fit(embeddings.tolist())
labels = ap.labels_

print_data_in_cluster(labels, 0)

clusters = len(np.unique(labels))
clusters

"""### DBSCAN"""

from sklearn.cluster import DBSCAN

db_clustering = DBSCAN(eps=2, min_samples=6).fit(embeddings.tolist())
labels = db_clustering.labels_

print_data_in_cluster(labels, 0)

"""### MeanShift"""

from sklearn.cluster import MeanShift, estimate_bandwidth

vectorizer = CountVectorizer(ngram_range=(1, 2))
data_vectorized = vectorizer.fit_transform(data).toarray()

bandwidth = estimate_bandwidth(embeddings.tolist(), quantile=0.9999, n_samples=500)

meanShift = MeanShift(bin_seeding=True)
meanShift.fit(embeddings.tolist())
labels = meanShift.labels_
n_clusters = len(np.unique(labels))

print(n_clusters)

